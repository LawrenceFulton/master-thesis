% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  ╭─────────╮                      ╔══╦╗ ╗ ╔═╦═╗ ╥ ┌─┐┌─┐┌─┐┬ ┬┌─┐┌┐ ┬
%  │ ,-= ━━━┑│                  |   ╠═╦╝╚╗╠╗║ ║ ╠═╣ ├─┤├─┤│  ├─┤├─ │└┐│
%  │ % iTec  │                  |   ╨ ╚═ ╚╝╚╝ ╨ ╨ ╨ ┴ ┴┴ ┴└─┘┴ ┴└─┘┴ └┘
%  │┃°. .°.  │ Chair Individual |          ┬ ┬┌┐ ┬┬┬  ┬┌─┐┬─┐┌─┐┬┌┬┐┬ ┬
%  │┖  °   ° │   and Technology |          │ ││└┐││└┐┌┘├─ ├┬┘└─┐│ │ └┬┘
%  ╰─────────╯                             └─┘┴ └┘┴ └┘ └─┘┴└─└─┘┴ ┴  ┴
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  This file is part of the Master's thesis LaTeX template used at the
%  Chair Individual and Technology (iTec) at RWTH Aachen University.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{(Theoretical) Background and Related Work}
\label{chap:background_related-work}


\section{Large Language Models}
\label{sec:llm}
\subsection{Model architecture}
Large Language Models (LLMs) are machine leaning models based on the Transformer Architecture which was first introduced by \citet{vaswani2017attention} and came proceeding developments in sequence modelling such as LSTMs \citep{hochreiter1997long} and (gated) Recurrent Neural Networks (RNN) \citep{cho2014learning}. Transformer models rely completely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolutions. These leads to major gains with regards to the parallelisation of computation and long-range dependencies of the input text \citep{kimDHR17}. 

In this process of exploring RNNs for machine translation and text summarisation the concept encoder-decoder structure was developed \citep{sutskever2014sequence}. The encoders will take the input being a embedding sequence $X$ which consists of the input and the positional encoding and creates an intermediate vector $Z$ which can then be processed by the decoder. Additionally the decoder will also get the previously self-generated output token as an input.


Building on the transformer architecture \citep{radford2018improving} created a new approach using the unsupervised pre-training + supervised fine-tuning paradigm. This is done by applying a generative pre-training on a the transformer, which is then followed by a discriminative fine-tuning on a specific task. The advantage of this was that due to the model already having the casual relationships inside one was able to fine-tune the model with a lot smaller datasets. Combining unsupervised with supervised techniques is in itself not novel and has already previously been used for sequence labelling or text classification \citep{van2020survey, yarowsky1995unsupervised}.


\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{imgs/decoder.png}
    \caption{Decoder structure proposed by \citet{radford2018improving}}
    \label{fig:decoder}
\end{figure}

Unlike encoder-decoder models, which process the entire input before generating output, GPT models rely only on a decoder with causal masking. This allows them to generate text sequentially while maintaining coherence over long contexts \ref{fig:decoder} \citep{radford2018improving,brown2020language}. The decoder will given sequence of tokens $(x_{i-k},...,x_{i-1})$ predict or classify token $x_i$. 



The attention mechanism contains 
The mentioned attention is symbolised by the formula \ref{for:attention}, where Q is the query, K is the Key and V is is value. 
\begin{align}
\label{for:attention}
    \text{Attention}(Q,K,V) &= softmax(\frac{QK^T}{\sqrt{d_k}})V
\end{align}


Rather then just having a single attention create a concept called multi-headed-attention which can be seen in formula \ref{for:multiHead} \citep{vaswani2017attention} . Having a multi-headed-attention allows for different attention heads to focus on different aspects of the input. This type of model also allows for highly parallelised training and inference of the data \citep{min2023recent}. 

\begin{align}
\label{for:multiHead}
    \text{MultiHead}(Q,K,V) &= Concat(head_1, head_2 ... , head_h)W^0 \\
    \text{where head}_i &= \text{Attention}(QW_i^Q,KW^K_i, VW^V_i)
\end{align}



\subsection{Alignment fine tuning RLTH}
\label{sub:alignment}

When using LLMs for generating text LLms will in their pre fine-tuned state find the token which has the highest likelihood of appearing. This behaviour might be different from the users expected behaviour which would often be that the LLM "follow the user’s instructions helpfully and safely" \citep{ouyang2022training}. This could result in "misaligned" behaviour such as making up facts, generating biased or toxic text, or simply not following user instructions \citep{bommasani2021opportunities, bender2021dangers, kenton2021alignment}, a term coined by \citep{ouyang2022training}.

To combat this misalignment they applied a system called reinforcement learning from human feedback (RLHF \citep{christiano2017deep}). This technique follows a three step procedure, firstly prompt taken from the API interface were taken and humanly annotated. This annotation was then used to train the model. Secondly, this model was then used to generate output based on another batch of prompts. In this step the human annotator would rank the output. This output was then used to generate a reward model. This is a much smaller (6B parameter in comparison to the 175B of GPT-3). Finally the LLM will be prompted again and the reward model will calculate the reward for this output. The reward is then used to update the policy using proximal policy optimization \citep{ouyang2022training}.

This technique used for fine-tuning resulted in many different positive results, such as that labellers significantly preferred the with RLHF fine-tuned model over the non-fined-tuned version even if the non-fined-tuned version was a lot larger, 

The model the researches applied RLHF on, InstructGPT, significantly outperforms the base model, GPT-3, in following instructions, with labellers preferring its responses. The 1.3B InstructGPT is rated higher than the 175B GPT-3, and the 175B InstructGPT is preferred 85\% of the time over GPT-3. It generates more appropriate, truthful, and constraint-following responses, reducing hallucination rates by about half. While InstructGPT slightly reduces toxicity, no effect was found with regards to social and gender biases\citep{ouyang2022training}.

\section{Bias in LLMs}
\label{sec:gender-bias-in-llm}

\subsection{Definition}

But what is social bias? Social Biases are very broad and can be observed in multiple different facets. In the taxonomy by \citet{gallegos_bias_2024} (table \ref{tab:harms}) social biases are subdivide it into two different groups. The first group representational harms contain misrepresentation, stereotyping, disparate system performance, derogatory language, and exclusionary norms and the second group allocational harms contain direct discrimination and indirect discrimination. It is important to notice that these groups are not exclusive and that the exclusionary norms may turn into allocational harms. For example by associating “Muslim” with “terrorist” could lead to an LLM-aided resume screening to hold preferences for non-Muslims.


\subsection{Sources of bias}

Bias is deeply embedded in the real world, shaping institutions, social structures, and digital spaces. Institutional biases often stem from historical and systemic inequalities, where certain social groups face disadvantages. The internet, despite its open accessibility, is not an accurate reflection of society. For example, Wikipedia's contributor demographics illustrate this disparity: while women make up 49\% of Wikipedia readers in the U.S., only an estimated 22.7\% of adult contributors are female \citep{hill2013wikipedia}. Such imbalances can be found in many other websites and are made more severe by phenomena like homophily (the tendency of individuals to interact with similar others \citep{karimi2018homophily}) and algorithmic feedback loops(reinforce biases through mechanisms like search engine ranking \citep{lerman2014leveraging}). Additionally, demographic factors such as age, education, and socioeconomic status further influence online engagement, shaping the data that algorithms rely upon.

LLMS inherit these biases from the data they are trained on. Modern AI systems require vast amounts of data, and their capabilities are directly tied to the information they are fed on. In the case of LLMs, training data is predominantly sourced from the internet, often through large-scale web crawls like Common Crawl \citep{brown2020language}. However, the web itself is skewed—only a fraction of individuals actively contribute content, meaning that the perspectives represented online are neither neutral nor fully inclusive. For instance, product reviews, forum discussions, and social media posts are authored by a self-selected subset of users, leading to an over-representation of certain opinions while others remain under-represented.

\subsection{Detection}

A wide variety of methods has been created to detect and measure the bias in LLMs. These generally fit into three categories, embedding based, portability based and Generated Text based \citep{gallegos_bias_2024}.


\subsubsection{Embedding Based Metrics (EBM)}
Here one looks at the vector embeddings of the words or of sentences. For example one could compare the cosine distance from the vector of the tokens for \texttt{doctor, nurse, man} and \texttt{woman}. With the assumption that unbiased embeddings should have similar cosine similarity to opposing social group terms  (Word Embedding Association Test \citep{caliskan2017semantics}). Modification from this, where one either takes the normalised sum of the vectors (Sentence Bias Score \citep{dolci2023improving}) or one takes the vector of the sentence (Sentence Encoder Association Test\citep{may2019measuring}) also exist. This technique comes with serious shortcomings since the the correlation between the found bias found in the embedding space and the bias found in the downstream application is very weak to non existent \citep{goldfarb2021intrinsic,cao2021holistic}. 

\subsubsection{Probability Based Metrics (PBM)}
Probability-based metrics assess bias by analysing how likely a model is to generate certain tokens or sentences. One approach would mask a part of the word and see which the likelihoods of words which would be chosen by the model \citep{webster2020measuring}. More sophisticated versions with normalisation also exist (Log-Probability Bias Score \citep{kurita2019measuring}).

Another group of methods, the Pseudo-Log-Likelihood Methods, such as the Context Association Test or CrowS-Pairs Score compare the likelihood of two sentences with a word changed. An example would be comparing the likelihood between "One wouldn't expect this discovery from a \textbf{Female} astrophysicist" and "One wouldn't expect this discovery from a \textbf{Male} astrophysicist". 

Again her only weak correlation can be found though between the PBM and the downstream task \citep{kaneko2022debiasing, delobelle2022measuring}. \citet{goldfarb2023prompt} observed that 68\% of the papers applying bias tests used EBMs or PBMs (only observing upstream tasks).

\subsubsection{Generated Text-Based Metrics}
\label{subsubsec:generated-text-based-metrics}    



The final category assess bias by analysing the outputs a model produces when prompted with different inputs. Thus treating the model as a black box.


\citet{gallegos_bias_2024} subdivided this into three different under-categories, distribution of output words, classifier and lexical based metrics. The Distribution-based metrics are used to analyse the distribution of tokens generated, by the LLM depending on the social group. The standard approach for this is that the LLM is prompted to, given a set of input tokens, a continuation of these. For example the Co-Occurrence score calculates the likelihood of words appearing more frequently in either female or male contexts \citep{bordia2019identifying}. There are a number of similar measurements which all work fairly similar such as the Demographic Representation \citep{liang2022holistic}, Stereotypical Associations \citep{liang2022holistic}, and Marked Persons \citep{cheng2023marked}.

The classifier sub-category of metrics use a second model to evaluate the output of the LLMs. Using a toxicity classifier one can evaluate either the mean toxicity score \citep{gehman2020realtoxicityprompts}, the ratio of outputs with a toxicity over a threshold \citep{liang2022holistic}. A similar approach can be used with regards to sentiment \citep{huang2019reducing,sheng2019woman}. The second model may also be trained on specific tasks, by generating a dataset based on different dimensions of sexism \citet{samory2021call} trained a logid, CNN, and fine-tuned a BERT language model. These resulting models all outperformed the baseline which was given by the Jigsaw’s Perspective API \citep{PerspectiveAPI2024} which is widely used  \citep{gehman2020realtoxicityprompts,liang2022holistic}

Third, lexical metrics provide a way to assess bias by analysing the language used in a model's generated output. Words can be mapped to predefined values, such as hurtfulness scores \citep{nozza2021honest}, psycholinguistic norms \citep{dhamala2021bold}, which include attributes like dominance, sadness, or fear, or the presence of gendered language \citep{dhamala2021bold}.

Each of these metrics come with associated shortcomings. With the distribution based metrics the co-occurrences between the input and protected attitude might be a poor proxy for downstream disparities. For example, co-occurrence can occur through a lot of counterspeach \citep{gligoric2024nlp}. The classifier categories will always only be as reliable as the model which is analysing the output. If there are biases in the auxiliary model the evaluation of the output might also have that bias. And with the final group, lexicon-based metrics, the issue exists that the lexicon can not understand words in the context.

A more recent approach evaluates bias through downstream task performance, rather than token-level distributions or lexical scores.  
For example, \citet{gupta2023bias} show that when LLMs are prompted to adopt socio-demographic personas (e.g., an Asian person, a physically disabled person), their reasoning performance on various tasks can drop substantially compared to a neutral persona. This demonstrates that bias can surface not only in the model’s language output but also in its functional behaviour, highlighting a complementary dimension to traditional generated text-based metrics.




\section{Multi-Agent Systems} 

Multi-agent systems (MAS) study interactions between autonomous agents that cooperate, compete, or coexist within a shared environment. A common definition describes MAS as "systems that include multiple autonomous entities with either diverging information or diverging interests, or both" \cite[p.xiii]{shoham2008multiagent}. These systems are widely applied in robotics, economics, artificial intelligence, and distributed computing \citep{wooldridge2009introduction}. Unlike single-agent models, MAS focus on coordination, negotiation, and decentralized decision-making, often drawing from game theory, logic, and reinforcement learning \citep{stone2010ad}.

Whereas early research on MAS explored rule-based and logic-driven frameworks \citep{shoham2008multiagent}, modern approaches increasingly focus machine learning and large language models for adaptive agent behaviour \citep{baker2019emergent}. The combination of LLMs and MAS has been widely explored recently with simulations ranging such as economical theories \citep{zhao2023competeai} or simulating real time social interaction \citep{kaiya2023lyfe}. 

When replicating psychological experiments with human participants LLM  have showed similar results \citep{cui2024can, aher2023using} to these. It is very important in this case to mention that LLMs are also trained on the experiments tested of these papers which make the results unsurprising. Some social phenomena are also not reproducible by using LLM agents. 

\subsection{Dyadic Debates}

One of these phenomena is the phenomena of the echo chamber theory. The echo chamber theory states that when individuals with shared opinions come together there views will intensify. Using digital trace data this has behaviour has been shown in numerous settings in Social Networks \citep{terren2021echo,mahmoudi2024echo, brugnoli2019recursive}. 

Similar to the experiment by \cite{taubenfeld_systematic_2024}, \cite{coppolillo_unmasking_2025} observed a shift away from the opinion in the echo chamber. Especially in cases where the opinions of the agents would represent a more conservative opinion. Both \cite{taubenfeld_systematic_2024} and \cite{coppolillo_unmasking_2025} interpret the results that LLM agents over the course of the debate will shift their opinions to be more in line with the LLM. 

The main difference between the approach by \cite{taubenfeld_systematic_2024} and \cite{coppolillo_unmasking_2025} is that instead of letting the agents self evaluate the opinions, \cite{coppolillo_unmasking_2025} chose to create a second type of agents (Opinion Presence Agent) which will read over the conservation and measures the stances of the LLMs. Furthermore, newer models were used such as \texttt{ChatGPT-4o}\footnote{https://openai.com/index/hello-gpt-4o/}.


\subsection{Left-Leaning Bias}

As both \cite{taubenfeld_systematic_2024} and \cite{coppolillo_unmasking_2025} noticed that the default opinions shifted more towards the liberal perspective the question arises if there is other evidence for a left-leaning bias. Recent studies have explored the political biases present in large language models. \citet{rutinowski2024self} found that LLMs tend to exhibit a left-leaning bias when within Chat-GPT when asked to answer the questions posed by the political compass test. Similar results can be seen when prompting LLMs the question of the "Wahl-O-Mart", a voting advice application used in Germany \citep{rettenberger2025assessing}. The authors prompted the LLMs to answer the questionnaire which resulted in the finding that Larger Models , tend to align more closely with left leaning political parties, while smaller models often remain neutral, particularly when prompted in English.


\section{Prompt Sensitivity}

LLMs are known to be highly sensitive to prompt design \citep{sclar2023quantifying, gao2020making, jiang2020can}. 
\citet{sclar2023quantifying} demonstrated that even minor, semantically irrelevant changes to prompt formatting can substantially affect model accuracy, shown clearly in their experiments with LLaMA-2-13B.

To systematically assess such effects, several frameworks have been proposed, including POSIX (PrOmpt Sensitivity IndeX) \citep{chatterjee2024posix} and ProSA (Prompt Sensitivity Assessment) \citep{zhuo2024prosa}. Both evaluate how much a model’s outputs vary when the input prompt is reformulated but its underlying intent remains constant.

The POSIX framework quantifies a model’s instability under semantically equivalent prompt variations. The authors argue that a robust LLM should behave consistently regardless of phrasing. Their evaluation includes three categories of prompt modification: (1) rephrasings preserving meaning, (2) changes in element order, and (3) stylistic adjustments such as tone or formality. The resulting POSIX score, used mainly for model comparison, illustrates how even minor changes (e.g., typos) can lead to large performance differences.

Similarly, ProSA focuses on stylistic prompt sensitivity, an aspect less studied in prior work. It applies two main classes of perturbations: substantial stylistic alterations (e.g., shifts in formality or tone) and finer character-level modifications. The framework then measures their effect on both output correctness and model confidence across various LLMs and tasks. 

ProSA’s analysis shows that larger models are not necessarily more robust to prompt variations—for instance, LLaMA3-8B-Instruct outperformed the larger Qwen1.5-72B-Chat on the sensitivity metric. Consistent with POSIX, few-shot prompting markedly reduced sensitivity, indicating that including even minimal contextual examples improves stability and output consistency.
